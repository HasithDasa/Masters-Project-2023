\documentclass[]{iat}
%Diese Beiden Pakete werden nur für das Beispieldokument benötigt und sollten von Ihnen gelöscht werden
\usepackage{listings}
\usepackage{scrhack}
\usepackage{algorithm2e}
\usepackage{placeins}
\usepackage{amsmath}
\usepackage[backend=biber,
style=numeric,
bibencoding=ascii
%style=alphabetic
%style=reading
]{biblatex}
%Hier können weitere benötigte Pakete Eingebunden werden

%Namen eingeben / Insert Name
\renewcommand{\author}{Dasanayake Mudiyanselage, Hasith Thilanka Dasanayake}
%Art der Arbeit / Scope (Project, Thesis...)
\providecommand{\scope}{Projektarbeit}
%Thema der Arbeit / Theme of Thesis
\renewcommand{\subject}{Cutting -line Segment Detection of Plant Sections}
%Schlagwörter / Keywords
\providecommand{\keywords}{Key_1, Key_2}
%Literaturliste *.bib / Bibliography
%providecommand{\bibfile}{IAT_Beispiel_literature}
%Matrikelnummer / Student ID
\providecommand{\studentID}{6003143}
%Betreuer /& Tutors
\providecommand{\tutora}{Philip Tietjen}
%Prüfer / Examiner
\providecommand{\examinera}{Prof. Dr.-Ing. Kai Michels}
\addbibresource{IAT_Beispiel_literature.bib}

\hypersetup{%
	pdftitle	={\subject -- \author -- \today},
	pdfauthor	={\author},
	pdfsubject	={\subject},
	pdfkeywords ={\keywords}
}


\setlength{\footheight}{21pt}

\begin{document}
	\lstset{literate={ä}{{\"a}}1}
%Pfad zu Grafiken:
	\graphicspath{{./project_graphics/}}
% Sprachauswahl /Language Selection (ngerman/english)
	\selectlanguage{english}
\pagenumbering{roman}
\input{iat_title}
%Urherberrechtserklärung / Confirmation of Conformity Comment if not needed, Select correct Language
\input{declaration_conformity}
\pagestyle{iat}
\tableofcontents
\chapter{Introduction}
\pagenumbering{arabic}
\setcounter{page}{1}
\par
In the field of agricultural processing, plant shoots are commonly exported to other countries today for preprocessing, specifically involving the cutting of the shoots into several sections. This process is time-consuming, relies on human intervention, and incurs high costs due to the subsequent need to reimport the processed plants for further processing. However, by employing the "GreenAI" system, which utilizes precise laser cutters, parallel robots, and a computer vision system, the cutting process of the plants can be executed with precision, cost-effectiveness, and efficiency \cite{greenai}.
\par
In order to divide the plant into multiple sections, it is necessary to accurately predict the exact cutting line segment of the plant. To achieve this, various techniques were employed. In this research project, the approach involved segmenting regions that intersect along the lines of interest, enabling the derivation of the actual lines indirectly from the predicted segments' intersections. A significant challenge encountered during the process was the accurate selection of the appropriate cutting line segment when one or more intersections appeared in two adjacent sections of the plant. Consequently, the following objectives were established:
\par

\begin{itemize}
    \item Identification of cutting line segments situated on the stem of the plant.
    \item Elimination of other non-relevant intersections that may appear as line segments, thereby selecting the correct cutting line segments.
    \item Analyzing the performance of the cutting-line segment detection algorithm.
\end{itemize}

\par
"Hasty," an annotation tool utilized in this project, assists in defining image object classes and generating ground truth datasets which were considered the root of the project\cite{CloudFactory}. This process is fundamental in a multitude of applications such as medical imaging, object detection, and autonomous driving. Many researchers found that the Run-length encoding approach allowed for efficient storage and quick data retrieval while maintaining the integrity of the original image which is generated by the Hasty \cite{samet1984,li2018}.
\par
Previous research on image preprocessing and feature extraction stages showed the importance of these processes in overall algorithm efficiency. For instance, S.D Gupta et al. emphasized the significance of preprocessing and feature extraction in the context of plant images, suggesting that these stages directly influence the analysis quality\cite{gupta2015}. Another study by S. Beucher and C. Lantuejoul depicted that these stages are integral for robust analysis, enhancing both the accuracy and efficiency of the detection algorithm \cite{beucher1979}.
\par
Edge detection is an important function of feature extraction in image processing. In order to achieve this various techniques were developed over the years. The Canny edge detection method, invented by Canny in 1986, remains a prominent approach\cite{Canny1986}. The application of this method has been widely documented. J.P.M. SOSA et al. used Canny edge detector to extract plant leaf shapes from an image file. Then it was used to identify possible relationship or the family of the plant using a desktop application \cite{inproceedings}. Also, X. Yan et al. implemented a method of lane edge detection based on the Canny algorithm for a vision-based vehicle assisting system. An advanced plant leaf classification through image enhancement and canny edge detection method was proposed by J. G. Thanikkal et al. It was directly influenced by this project \cite{8748587}. Also, Canny edge detection was used in several industrial applications like coal mining, packaging, and production etc \cite{8243122, 9853348, beeran2014evaluation}.
\par
When canny failed, alternative techniques were considered. Consequently, the Sobel operator is used for edge detection based on the gradient magnitude of an image, highlighting areas of significant intensity changes \cite{duda2000pattern}. The edge detection of license plates and logo edges, etc, has been performed using the Sobel operator \cite{7755367,beeran2014evaluation}. Moreover, comparison analyses have been conducted to identify the most effective edge detection algorithm. Based on these studies, Sobel is deemed suitable for images afflicted with random noise. However, the Sobel operator's smoothing effect can remove important features from the image. But the main advantage of the Sobel operator is its ability to detect edges more quickly \cite{7938931,9232632}. This was considered for this project. 
\par
Another technique that can be considered is the Laplacian of Gaussian (LoG) method, which combines the Laplacian operator and Gaussian smoothing to emphasize areas of rapid intensity changes \cite{canny1980}. In a detailed discussion by P. Dubey et al., the LoG operator, Canny operator, and a hybrid version of these two operators were extensively analyzed using metrics such as Mean Square Error, Peak Signal-to-Noise Ratio, etc \cite{9170672}. 
\par
In order to address the complexity of our dataset, it was determined that a sequential approach using the aforementioned operators would be suitable. Specifically, if one operator fails to detect any edges, the subsequent operators are applied in a cascaded manner to increase the detection. This approach ensures a stable edge detection process, improving the overall performance and reliability of the system.
\par
Line segment detection has been addressed through various methods in the computer vision domain. The Hough transform is a method initially introduced in the 1960s for line detection \cite{Hough_1962_2186}. Also to overcome the limitations of the Hough transform an alternative approach called the probabilistic Hough transform, has been developed. It utilizes a randomized subset of edge points instead of considering all edge points\cite{Matas2000}. This modified version of the original Hough Transform significantly reduces computation time. These two methods have been widely applied in numerous applications involving line segment detection, such as lane tracking in autonomous vehicles\cite{10025098}, Optical character recognition\cite{243184}, and power line tracking in the Electric power industry\cite{10060117}, among others. 
\par
However, in this project, it was observed that less accurate results were obtained using the above standard methods due to the presence of small line segments and non-linear segments. Therefore, a requirement of designing a specific algorithm was concerned to extract the correct line segments.
\par
The Line Segment Detector (LSD) algorithm proposed by Von Gioi et al. provides a robust and accurate method for detecting line segments\cite{ipol.2012.gjmr-lsd}. One advantage of LSD is that it does not require tuning parameters, making it easier to use than the Hough Transform. This method was used partially in this project to detect the stem of the plant separately in the classification chapter. Also, very common computer vision operations like morphological operations which were used in many agricultural applications to detect plants\cite{7418873, 10051285} were used to fine-tune the results of plant stem detection. 
\par
The edge detection algorithm’s performance evaluation primarily focused on the use of various metrics, namely the confusion matrix, accuracy, precision, F1 score, and the receiver operating characteristic (ROC) curve \cite{powers2011evaluation}.
\par
These evaluation techniques have been mainly employed in numerous applications. Ray et al. employed the confusion matrix to assess the effectiveness of their machine-learning algorithm in predicting crop yield in the agricultural domain \cite{9734173}. Furthermore, Nagaraj et al. and Upadhyay et al. conducted a performance analysis of several supervised learning algorithms, such as XGB, LGBM, KNN, SVM, and LR, in the domains of breast cancer risk detection and heart disease prediction respectively \cite{10080076, 9537093}.
\par
Additionally, Hanley and McNeil pointed out, the area under the ROC curve (AUC-ROC) serves as a main indicator of a model's performance due to its consideration of various thresholds, in contrast to precision and recall \cite{doi:10.1148/radiology.143.1.7063747}
\par
Finally, in this project, 316 samples were used to analyze the performance of the algorithm based on the aforementioned metrics. It was broadly discussed in the Results chapter.
\par
Big O notation is widely used in computer science to describe the performance of an algorithm as the input size increases. In this project, the efficiency of the algorithm was analyzed using this notation. The concept was initially explored by G. Brassard and P. Bratley in their work, wherein they emphasized its significance in comparing algorithm performance \cite{brassard1996fundamentals}. To discover the efficiency of Euler Arithmetic's algorithm complexity, N. Samsudin employed Big O notation. This involved analyzing the algorithm's runtime to determine its efficiency. Furthermore, a comparison was made between the algorithm's complexity using O(1) and O(n) notations \cite{Samsudin2020}. Our project adopted a similar approach in order to determine the appropriate Big O notation.
\par
\chapter{Methodology}
Line Segment Detection (LSD) is the most common and essential step in computer vision. The major challenges in LSD such as accurate segment selection out of several segments, and the efficient processing of the algorithm, have been extensively studied.
\par
In this research, Run-Length Encoded (RLE) and annotated images in JSON format were used as the initial data source for the implementation of the Line Segment Detection Algorithm. After RLE data has been decoded to an image, preprocessing and preparation steps were described in subtopic 2.1. Then in subtopic 2.2, the feature extraction step was described. Which is the most important step to detect all the line segments in the image. In the final step, (subtopic 2.3) correct line segments between two neighboring plant sections were successfully identified on the stem of each plant. \autoref{fig:fig1} shows the methodology in a flow chart.

\FloatBarrier
\begin{figure}[h]
    
	\includegraphics[width=\textwidth]{project_graphics/Fig1.eps}
	\caption{Flow Chart of the Overall Algorithm }
	\label{fig:fig1}
\end{figure}

\section{Preprocessing and Preparation}
In this project, an image annotation tool called “Hasty” was used to create a ground-truth dataset. During the annotation, the object classes of the image were defined as “First Section Cutting”, “Redundant Top End” and “Tip Cutting” etc. Finally, the annotated image was encoded to a run-length encoded (RLE) binary mask, which is a compressed representation of a binary image. In this encoding, the binary image is represented by a sequence of pairs (start, length), where each pair represents a consecutive run of 1's in the image. 
\par
In the 1st part of the algorithm, this mask was decoded and created a 2D numpy array. Also, image dimensions, bounding box details, and selected object classes (First Section Cutting, Redundant Top End, Redundant Bottom End, Tip Cutting, Non-Viable Part, Second Section Cutting, Third Section Cutting, Fourth Section Cutting) which are useful for the line segment detection were gathered in this stage.
\par
Usually, sample images contain one or more plants, and in the second part of the algorithm, two Python lists were generated. Typically, in the Hasty Generated JSON file, plant sections are annotated with specific small bounding boxes and saved separately. The exact coordinates of these small bounding boxes were stored in the first Python list (Let’s call “small\_bbox\_list”). Also, large bounding boxes were defined for each plant (with all the section cuttings) in the JSON file and saved separately in the second Python list (Let’s call “large\_bbox\_list”). In addition, a dictionary was created to store the details of each plant section. In this case, the background of the image was saved as "0" and the foreground as "1". As shown in the  \autoref{fig:fig2} one large bounding box consisted of multiple small bounding boxes. 
\par
\FloatBarrier
\begin{figure}[h]
    
	\includegraphics[width=\textwidth]{project_graphics/Fig2.eps}
	\caption{Small and Large Bounding Boxes}
	\label{fig:fig2}
\end{figure}


Then the 3rd part of the algorithm was used to create the list of neighboring sections of the plants. As an example, 1st Section and Redundant Bottom End section of the plant. This part of the algorithm is more challenging because the efficiency of the algorithm mainly depends on this section. Here, OpenCV function: cv2.findContours() was used to detect only the external contours (cv2.RETR\_EXTERNAL) of the sections of the plants and saved them in a list(Let’s call “edge\_only\_list”). Then in order to reduce the memory usage and speed up the process “cv2.CHAIN\_APPROX\_SIMPLE” method was used as the contour approximation method. Initially, both internal (whole solid section of the plant) and external contours were used to find the neighboring sections of the plants. But during the stage of optimizing the algorithm’s efficiency, huge efficiency improvements were achieved while using this outside contour detection method. It was discussed more in the results section.
\par
Also, an additional major improvement in efficiency was achieved through the implementation of parallel processing. The most popular two types of parallel processing methods were tested, which were based on multiple threads and multiple processors. The use of multiple processors for parallel processing involves the distribution of the workload across numerous physical or logical processors. Two separate functions were used in the algorithm (function for the identification of neighboring sections and function for the identification of complete sections of the plant) to facilitate easy parallel processing. For initial testing, a standard Python library called "multiprocessing" was employed. Regrettably, the processing time exceeded the duration of normal processing time. Consequently, an alternative technique based on multiple threads was adopted, utilizing the "concurrent.futures" module in Python.
\par
A thread is a lightweight unit of execution that can run concurrently with other threads, sharing the same memory space. As shown in the following  \autoref{alg:algo1}, \autoref{alg:algo2},  \autoref{alg:algo3} , multi-threading was used. After that, identical combinations of neighboring sections were eliminated. For instance, combinations such as (1st section cutting, redundant bottom end cutting) and (redundant bottom end cutting, 1st section cutting) were considered similar combinations and were removed from the list of neighboring sections.
\par
\FloatBarrier
\begin{algorithm}[htbp]
    \SetAlgoLined
    \caption{Finding Neighboring Sections}
    \label{alg:algo1}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{small\_bbox\_list, edge\_only\_list}
    \Output{neighboring\_sections\_list, intersected\_area\_list}

    \BlankLine
    Initialize an empty list neighboring\_sections\_list; Initialize an empty list intersected\_area\_list\;

    \BlankLine
    \ForEach{mask\_index in small\_bbox\_list}{
        \ForEach{mask\_index\_checked in edge\_only\_list}{
            \ForEach{ones\_index in edge\_only\_list[mask\_index\_checked]}{
                \If{(\(small\_bbox\_list[mask\_index][0] \leq edge\_only\_list[mask\_index\_checked][ones\_index][1] < small\_bbox\_list[mask\_index][1]\)) \textbf{and} (\(small\_bbox\_list[mask\_index][2] \leq edge\_only\_list[mask\_index\_checked][ones\_index][0] < small\_bbox\_list[mask\_index][3]\))}{
                    \If{mask\_index $\neq$ mask\_index\_checked}{
                        Append [mask\_index, mask\_index\_checked] to neighboring\_sections\_list; Append [(edge\_only\_list[mask\_index\_checked][ones\_index][0], edge\_only\_list[mask\_index\_checked][ones\_index][1]), mask\_index, mask\_index\_checked] to intersected\_area\_list\;
                    }
                }
            }
        }
    }

    \BlankLine
    \Return(neighboring\_sections\_list, intersected\_area\_list)\;
\end{algorithm}

\FloatBarrier

\begin{algorithm}[htbp]
    \SetAlgoLined
    \caption{Finding Small BBoxes Inside Large BBox}
     \label{alg:algo2}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{large\_bbox\_list, edge\_only\_list}
    \Output{mask\_inside\_large\_bbox\_list}

    \BlankLine
    Initialize an empty list mask\_inside\_large\_bbox\_list\;

    \BlankLine
    \ForEach{lg\_bb\_ind and lg\_bb\_ele in large\_bbox\_list}{
        \ForEach{mask\_index\_2 and mask\_ele\_2 in edge\_only\_list}{
            Initialize ones\_count as 0\;

            \ForEach{co\_with\_ones\_ele2 in mask\_ele\_2}{
                \If{(\(lg\_bb\_ele[0] \leq co\_with\_ones\_ele2[1] < lg\_bb\_ele[1]\)) \textbf{and} (\(lg\_bb\_ele[2] \leq co\_with\_ones\_ele2[0] < lg\_bb\_ele[3]\))}{
                    Increment ones\_count by 1\;
                }
            }

            \If{ones\_count is equal to the length of mask\_ele\_2}{
                Append [lg\_bb\_ind, mask\_index\_2] to mask\_inside\_large\_bbox\_list\;
            }
        }
    }

    \BlankLine
    \Return(mask\_inside\_large\_bbox\_list)\;
\end{algorithm}

\FloatBarrier
\begin{algorithm}[htbp]
    \SetAlgoLined
    \caption{Parallel Processing}
    \label{alg:algo3}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{small\_bbox\_list, large\_bbox\_list, edge\_only\_list}
    \Output{results}

    \BlankLine
    Create an executor using ThreadPoolExecutor\;

    \BlankLine
    Submit the first for loop as a task\;
    task1 = executor.submit(finding\_neighbors, small\_bbox\_list, edge\_only\_list)\;

    \BlankLine
    Submit the second for loop as a task\;
    task2 = executor.submit(finding\_sections, large\_bbox\_list, edge\_only\_list)\;

    \BlankLine
    Wait for both tasks to complete\;
    results = [task1.result(), task2.result()]\;

    \BlankLine
    Combine the results from both tasks and return them\;
    \Return(results)\;
\end{algorithm}

\section{Feature Extraction}

The feature extraction step is the most important step in this project because the cutting line segment of two neighboring sections is situated on the edge. Therefore, several edge detection methods were tested to get optimum results.
\par
In this project, the most important step is the feature extraction step due to the placement of the cutting line segment of two neighboring sections on the edge. Consequently, several edge detection methods were employed to achieve optimum results.
\par
The initial detection method employed in this study was the Canny Edge Detection method (by using Opencv based algorithm (cv2.Canny)), which theoretically encompasses several steps: Firstly, a Gaussian filter was utilized to smooth the image and reduce noise. Subsequently, the gradients of the image intensity were calculated using the Sobel operator. To thin the edges and keep only the maximum values, non-maximum suppression was applied. Pixels were classified as strong, weak, or non-edges through the utilization of double thresholding. Finally, weak edges that are connected to strong edges are retained as actual edges using a process called edge tracking by hysteresis. 
\par
During the analysis of the results, certain line segments were not identified. Therefore, an alternative method was deployed to identify these missing line segments. In this study, the Sobel operator (cv2.Sobel) was applied in both the x and y directions, and the results were merged. Usually, the Sobel Operator is used to compute the gradient magnitude and direction of an image, facilitating the detection of regions exhibiting substantial changes in intensity. 
\par
Moreover, the identification of edges was performed using the Laplacian of Gaussian (LoG) method. The regions of rapid intensity changes were highlighted by applying a combination of the Laplacian operator (cv2.Laplacian) and Gaussian smoothing (cv2.GaussianBlur) with a kernel size of 3x3. After that, the results obtained from two neighboring plant sections were subjected to an AND operator. 
\par
Finally, various types of line segments were revealed. The majority of the line segments were on the stem, although some irrelevant lines were observed in the area where two leaves intersected with each other or with the stems. (see \autoref{fig:fig3}).
\par
\FloatBarrier
\newpage
\begin{figure}[h]
    
	\includegraphics[width=\textwidth]{project_graphics/Fig3.eps}
	\caption{Intersection of Two Leaves and Leaf and Stem in Two Neighboring Sections}
	\label{fig:fig3}
\end{figure}

\section{Detection of Exact Cutting Line Segment (Classification)}

When not all line segments are situated on the stem of the plant, then it's a bit challenging to identify the correct segment. Therefore, the detection of the exact line segment was performed in three steps based on the results of the feature extraction stage. Initially, a combination of nearest sections with single line segments was considered(first step). Subsequently, combinations of nearest sections were identified with multiple line segments, which were further classified into two main categories. (These line segments arise primarily from the intersection of two leaves or the intersection of a portion of the leaves and stems). The first classification (second step) was conducted based on the size of the line segments. Data analysis revealed that the majority of the samples depicted a noticeable difference in size, although some samples showed line segments that were nearly similar in size. For this type of sample, the second classification (third step) was employed using the distance of the line segments to the stem.
\par
The first step was easily achieved by clustering of the adjacent foreground pixels because only a single line segment needed to be detected. A threshold value was defined to categorize the cluster of adjacent pixels with non-adjacent pixel clusters. If there is another cluster that is above the threshold value, the index of that cluster's closest pixel were stored in a list called "clustering\_list". (Here, nothing was stored in the "clustering\_list" because all the pixels were adjacent and there was no more than one cluster in this step).  Throughout the first step, only one cluster of adjacent pixels (one line segment) was consistently detected. Finally, all the pixel coordinates were saved in another list called "cluster\_coor\_list", which served as the final output (detected line segment) of the algorithm.
\par
The second step is executed if the length of the "clustering\_list" is found to be non-zero which means multiple line segments are detected. Here, the "cv2.findContours" method was once again used to detect all available common line segments in two neighboring plant sections. The size of each line segment was measured using "cv2.arcLength()" to identify the range of sizes typically observed on the stem, as the cutting line. Based on this range, the correct line segment was detected. As before, the pixel coordinates were saved in the "cluster\_coor\_list".
\par
The third step is pursued only if the size of the line segment falls outside the size range or if there are line segments with similar lengths. In this scenario, several methods were attempted to identify the stem of the plant, with the primary concept being to measure the distance from a point in the stem to the nearest line segment.
\par
Initially, a Python library called "PlantCV," designed for image analysis in the field of plant research and based on OpenCV, was utilized to detect the stem\cite{PlantCV}. The "plantcv.morphology.skeletonize()" function was applied to obtain the skeletonized image, extracting the skeleton while preserving the plant structure's connectivity. However, a major challenge arose when applying skeletonization to the binary masks representing the plant sections due to the indistinguishable boundary between leaves and stems. Then, the algorithm occasionally misidentified leaves as stems and skeletonized them as well. Ultimately, the desired separate classification of stems and leaves  was not achieved.
\par
For the same purpose, the next approach involved segmenting the edge of the stem into multiple line segments. The "cv2.createLineSegmentDetector" function was employed, and the result was saved as "LSD\_img". Based on the outcomes, parallel or nearly parallel line segments were observed on either side of the stem. If two or more parallel line segments were sufficiently thickened, the possibility of creating a single line or solid object arose. This technique was utilized to detect stems separately. Consequently, these line segments were dilated using the "cv2.dilate" function with a 7x7 kernel (numpy.ones((7, 7), dtype=np.uint8)) and two iterations. To remove unwanted parts, an erosion operation was applied using the OpenCV erosion function with the same configurations as the dilation function. The difference between the result ("dilated\_eroded\_img") and "LSD\_img" was saved as "subtracted\_img" and subjected to a median blur operation with a kernel size of 7 to reduce noise and outliers.
\par
The resulting image ("subtracted\_img") consisted of disconnected parts of the stem due to improper boundaries where the leaves join. Therefore, another morphological dilation operation was performed to connect those areas as much as possible. Subsequently, the closed and largest contour was assumed to represent the stem, while other small, closed contours were disregarded. The Euclidean distance from that contour to the line segments was calculated, and the line segment with the shortest distance was considered the correct cutting line segment. Finally, the pixel coordinates of that line segment were saved in the "cluster\_coor\_list".
\par
\chapter{Results and Analysis}

In this section, the results of the algorithm for detecting accurate cutting line segments are presented under several subtopics. A JSON file was used, containing a total of 316 images along with varying numbers of plants captured in each image. As an example, in some images, 21 section cuttings were available, while in others, only one or two were available. 
\par
In subtopic 3.1, a selected sample image was considered, and it served as an illustrative example, displaying all the algorithm steps employed. In subtopic 3.2, the accuracy and precision of the algorithm had been evaluated. Furthermore, the efficiency of the algorithm had been considered a critical factor to measure the quality of algorithm. This aspect has been thoroughly described in subtopic 3.3
\par

\section{Visualization of the Algorithm Steps}

As shown in the \autoref{fig:fig4} , all the steps of the algorithm were visualized with the help of an illustrative example(stn2\_pkg004\_0\_1077\_rep). In the feature extraction stage, of the \autoref{fig:fig4} detected line segments were circled in yellow. The details of the  Classification stage were summarized in the \autoref{tab:tab1}. For the index [0, 1], coordinates were received as (887, 1076), (888, 1076), (889, 1077), (890, 1077), (892, 1079), (893, 1079), (892, 1078), (891, 1078), (890, 1077), (889, 1077), (888, 1076). Then for the index [1,2], coordinates were received as (935, 787), (936, 787), (937, 788), (936, 787). 

\par
\FloatBarrier
\begin{figure}[h]
    
	\includegraphics[width=\textwidth]{project_graphics/Fig4.eps}
	\caption{Visualization of Algorithm Steps}
	\label{fig:fig4}
\end{figure}

\FloatBarrier
\begin{table}[h]
\begin{tabular}{crl}
\hline
Neighboring Sections & Index & Classification\\\hline
1st Section and Redundent Bottom End & [0, 1] & Using Size \\
1st Section and Raw Cutting & [1, 2] & Using Size \\\hline
\end{tabular}
\caption{Line Segment Coordinates}
\label{tab:tab1}
\end{table}
\section{Evaluation of Algorithm’s Performance }

As mentioned in the introduction, evaluation of this algorithm's performance was done by using 316 samples. Through the analysis of various evaluation metrics and the visualization of results, insights can be gained into how well positive and negative instances are accurately identified by the algorithm. In this project, a fundamental tool for summarizing classification results called a confusion matrix was used. Moreover, the calculation and interpretation of key evaluation metrics such as accuracy, precision, and the F1 score were examined. Furthermore, the Receiver Operating Characteristic (ROC) curve and its significance in assessing the algorithm's performance in terms of Recall and false positive rates were investigated. 
\par
The confusion matrix is a tabular representation of the performance of the algorithm by the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) instances. Then it was used as the basis or the foundation of finding other evaluation metrics. 
\par
As shown in the, the actual classes of “correct line segment available and correct line segment not available” are represented by the columns, while it’s detected(predicted) or not detected are represented by the rows. The number of instances falling into each category is correspondingly indicated by the values in the cells. 
\par
\FloatBarrier
\begin{figure}[h]
    
	\includegraphics[width=0.5\textwidth]{project_graphics/Fig5.eps}
	\caption{Confusion Matrix}
	\label{fig:fig5}
\end{figure}

The evaluation metrics were computed using the values derived from the confusion matrix. These metrics provide an overall assessment of the algorithm's performance. 
\par
The following formula was used to calculate the accuracy, which measures the overall correctness of the algorithm's predictions.
\FloatBarrier
\begin{align}
Accuracy &=\frac{(TP+TN)}{(TP+TN+FP+FN)}
\end{align}
Initially, the accuracy of the algorithm was calculated with only two steps. Only the 1st step and 3rd detection steps of the algorithm (see subtopic 2.4). Then the accuracy was detected as 0.8912(89.12\%). As shown in the \autoref{fig:fig6} after implementing all three detection steps (see subtopic 2.4) to the algorithm, the accuracy of all the samples was plotted, and the average accuracy was taken as 0.9616 (96.16\%). Therefore, an improvement of 0.0704(7.04\%) was achieved by expanding the detection step. 
\par
\FloatBarrier
\begin{figure}[h]
	\includegraphics[width=\textwidth]{project_graphics/Fig6.eps}
	\caption{Accuracy vs Image Index Graph}
	\label{fig:fig6}
\end{figure}
\par
The classification algorithm's overall predictions were highly accurate, according to this accuracy score. Also, the algorithm demonstrates its effectiveness in distinguishing between positive and negative instances (detection and non-detection).
\par
However, a comprehensive analysis of other evaluation metrics is necessary to gain a proper understanding of the algorithm's performance, as well as its strengths and limitations. Therefore, precision and the F1 score were further calculated.
\par
The precision metric evaluates the algorithm's ability to avoid false positive predictions by measuring the proportion of correct positive predictions out of all instances predicted as positive. The following formula represented the precision in terms of the confusion matrix.
\FloatBarrier
\begin{align}
Precision &=\frac{TP}{(TP+FP)}
\end{align}
\par
\FloatBarrier
\begin{figure}[h]
    
	\includegraphics[width=\textwidth]{project_graphics/Fig7.eps}
	\caption{Precision vs Image Index Graph}
	\label{fig:fig7}
\end{figure}
\par
As shown in the \autoref{fig:fig7}. after implementing all three detection steps, the precision of all the samples was plotted and the average precision was taken as 0.9608 (96.08\%). As a result, the occurrence of false positives was minimized and making it reliable in distinguishing positive instances from negative ones.
\par
The F1 score, which is determined using the equation provided, represents a balanced measure of the algorithm's performance as it combines both precision and recall in a harmonic mean. These two metrics offer an overall assessment of the algorithm's ability to correctly identify positive instances while minimizing false positives and false negatives.
\par
\FloatBarrier
\begin{align}
F1\_score &=\frac{2*(Precision*Recall)}{(Precision+Recall)}
\end{align}

The average F1 score was achieved as (0.9785) 97.85\%, which indicates that the algorithm achieves a high balance between precision and recall. Here, recall for all the samples was calculated as one because there were zero instances in which the correct line segment was not detected. (Unless it was not passed from the feature extraction section to the classification section of the algorithm.)
\par
As depicted in the \autoref{fig:fig8} higher F1 score was shown. It shows that the algorithm is performing well in terms of correctly identifying positive instances (exhibiting high precision) and minimizing false negatives (demonstrating high recall). 
\par
\FloatBarrier
\begin{figure}[h]
    
	\includegraphics[width=\textwidth]{project_graphics/Fig8.eps}
	\caption{F1 Score vs Image Index Graph}
	\label{fig:fig8}
\end{figure}
The Receiver Operating Characteristic (ROC) curve was constructed by plotting the true positive rate (Recall) against the false positive rate (FPR) at various classification thresholds. For best-performing algorithms, the top-left corner of the graph would be touched by a ROC curve.
Here it was achieved as shown in \autoref{fig:fig9}
\par
\FloatBarrier
\begin{figure}[h]
	\includegraphics[width=\textwidth]{project_graphics/Fig9.eps}
	\caption{ROC Curve}
	\label{fig:fig9}
\end{figure}
\par
In summary, the algorithm's reliability and effectiveness in classification tasks are signified by the high F1 score, accuracy, and precision values. Furthermore, the perfect sensitivity suggested by the straight line in the ROC curve and the absence of false negatives further affirms the strong performance of the algorithm.
\par
\section{Efficiency Analysis of the Algorithm}

The efficiency analysis of the algorithm is based on the processing time of the algorithm. As mentioned in the methodology, there are main three sections in this algorithm. But for the efficiency analysis, time spent on preprocessing and the feature extraction were combined into one single section and called “initial preparation time”. The time spent for the classification step was divided into three sections “single line detection”, “detection based on the size” and “detection based on the distance”. To measure the processing time, the Python library “timeit” was used.
\par
Based on the results, the most time-consuming area in the algorithm was identified as the preprocessing and the feature extraction area. Initially, processing time was compared by changing the hardware configurations. For this, two completely different computers were used to run the program and capture the processing time as shown in the \autoref{tab:tab2}. 1st configuration was Intel Core i5 - 9300H CPU @ 2.40GHz Processor (9th Gen), 16.0 GB RAM, 512 GB SSD Harddisk, and 2nd configuration was Intel Core i5 - 4210U CPU @ 1.70GHz Processor (4th Gen), 8 GB RAM, 750 GB SATA Harddisk.
\par
\FloatBarrier
\begin{table}[h]
\begin{tabular}{crl}
\hline
Configuration & Min Proc. Time (ms) & Max Proc. Time (ms)\\\hline
1st configuration & 66.42  & 716.73 \\
2nd configuration & 103.24 & 2078.21 \\\hline
\end{tabular}
\caption{Performance of the Algorithm}
\label{tab:tab2}
\end{table}
According to the \autoref{tab:tab2}, a huge contrast in the processing time was noticed. Usually, the CPU is responsible for executing instructions and performing calculations. The 1st computer has a higher clock speed (2.40GHz) compared to the 2nd computer. A higher clock speed generally means faster processing, Therefore, one reason for the less processing time in the 1st computer is the performance of the CPU. In the computer architecture, RAM is used to store data that the CPU needs to access quickly. Having more RAM allows this kind of complex algorithm to be processed efficiently. The first computer has 16GB of RAM, which is twice the capacity of the second computer's 8GB. This means that the first computer can handle larger amounts of data in memory, potentially leading to faster algorithm execution. The storage drive, whether it's an SSD or an HDD, affects the algorithm's speed primarily during data read/write operations like loading JSON files, etc. SSDs are generally faster than traditional HDDs, offering quicker data access times. The first computer's 512GB SSD is likely to provide faster read/write speeds compared to the second computer's 750GB SATA drive. Therefore, the 1st computer was taken into further processing. 
\par
When further analyzing the preprocessing and feature extraction stages, a hypothesis was formulated to examine the impact of the number of neighboring plant section combinations (in a sample image) on the efficiency of the algorithm. The relationship between these two parameters was investigated using Big O notation, a mathematical notation commonly used in computer science. Big O notation, denoted as O(f(n)), where "O" represents the order of growth and "f(n)" denotes the algorithm's growth rate as a function of the input size "n," is used to analyze and compare algorithm performance as the input size increased. Typically, the function "f(n)" represents the worst-case time complexity of the algorithm, providing a clear indication of its efficiency with increasing input size.
\par
Based on the algorithm's performance behavior in relation to the input size, various types of Big O notations have been defined. For instance, O(1) represents constant performance, O(log n) denotes logarithmic performance, and O(n) signifies linear performance. Considering the processing structure of this algorithm, it was theoretically assumed that the algorithm's Big O notation complexity should be linear (O(n)). To validate this assumption, a graph was plotted, as depicted in \autoref{fig:fig10}, representing the algorithm's processing time against the number of neighboring plant section combinations. Then the graph was examined to identify patterns in the growth of the processing time. Additionally, to compare the algorithm's performance, a processing time reduction technique (refer to subtopic 2.2) utilizing the external contour detection method(this is called "Optimized" graph in the \autoref{fig:fig10})instead of the internal contour detection method (utilizing the whole solid section of the plant) was used as shown in the \autoref{fig:fig10}.
\par
\FloatBarrier
\begin{figure}[h]
	\includegraphics[width=\textwidth]{project_graphics/Fig10.eps}
	\caption{Process Time vs No. of Neighboring Plant Section Combinations }
	\label{fig:fig10}
\end{figure}

Observing \autoref{fig:fig10}, significant differences in the growth rate of the processing time (gradient) were noticed as the number of combinations increased. The relevant Big O notation or the relationship between the number of combinations and the processing time was summarized in the \autoref{tab:tab3}. This finding is explained by the fact that the number of combinations increases in an image, the size of the plant, the plant's section size, and the size of the outside contour decreases. Therefore, less time is required for processing the contours. This is further explained in the following graph. (See \autoref{fig:fig11}). 
\par
\FloatBarrier
\begin{table}[h]
\begin{tabular}{crl}
\hline
Range of Combinations & Relationship\\\hline
1 to 4 Not Optimized & O(n) = 182.76n + 1952.4 \\
5 to 9 Not Optimized & O(n) = 195.1n + 1644.7 \\
10 to 21 Not Optimized & O(n) = 52.115n + 2779.2 \\
1 to 4 Optimized & O(n) = 47.496n + 629.27 \\
5 to 9 Optimized & O(n) = 62.597n + 495.79 \\
10 to 21 Optimized & O(n) = 16.404n + 935.39 \\\hline
\end{tabular}
\caption{Big O Notation ("O(n)" represents processing time and "n" represents No. of combinations)}
\label{tab:tab3}
\end{table}
\par
\FloatBarrier
\begin{figure}[h]
	\includegraphics[width=\textwidth]{project_graphics/Fig11.eps}
	\caption{Average Time per Combination vs No. of Neighboring Plant Section Combinations }
	\label{fig:fig11}
\end{figure}
Also, a logarithmic relationship was found in the average time per combination vs No. of combinations. 
\FloatBarrier
\begin{align}
\intertext{"T" represents the average time per combination and "N" represents the number of combinations(N>0)}
T &=-167.2ln(N)+504.12
\end{align}
Also based on the optimized algorithm utilizing the external contour detection method, roughly 3 times the processing time reduction was achieved due to less amount of pixels available on the external contour compared to the whole solid section of the plant.
\par
 A visual representation of single-line detection is provided in \autoref{fig:fig12}, where the histogram illustrates a left-skewed distribution. This signifies that the range of time dedicated to detecting a single line is from 39ms to 239ms.
\par
\FloatBarrier
\begin{figure}[h]
	\includegraphics[width=\textwidth]{project_graphics/Fig12.eps}
	\caption{Histogram - Detection using Only Single Line  }
	\label{fig:fig12}
\end{figure}
\par
With respect to the detection based on size, as depicted in\autoref{fig:fig13}, the time distribution follows a similar left-skewed pattern. Comparable to the previous instance, the duration for this type of detection also lies in a similar range, that is, between 41ms to 241ms.
\par
\FloatBarrier
\begin{figure}[h]
	\includegraphics[width=\textwidth]{project_graphics/Fig13.eps}
	\caption{Histogram - Detection using Size  }
	\label{fig:fig13}
\end{figure}
\par
However, a deviation can be observed when we examine the time spent for detection based on distance, as represented in \autoref{fig:fig14}. In this particular scenario, the duration is slightly increased, ranging from 286ms to 486ms, which is higher than the time required for the other two forms of detection.
\par
\FloatBarrier
\begin{figure}[h]
	\includegraphics[width=\textwidth]{project_graphics/Fig14.eps}
	\caption{Histogram - Detection using Distance  }
	\label{fig:fig14}
\end{figure}
\par
Therefore, it can be interpreted from the histograms that while the time distributions for single-line detection and size-based detection are quite similar and relatively lower, the time required for distance-based detection is marginally greater.
\par
It may be worth considering whether the increased time for distance-based detection is due to the complexity of the algorithm, or whether it could be optimized further. Understanding all of these factors could help to determine which detection method has to be improved in the future. 
\par
\chapter{Conclusion and Discussion}
This is under construction

\printbibliography

\end{document}
